{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandrerame/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
      "  DeprecationWarning)\n",
      "/Users/alexandrerame/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
      "  DeprecationWarning)\n",
      "/Users/alexandrerame/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
      "  DeprecationWarning)\n",
      "/Users/alexandrerame/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
      "  DeprecationWarning)\n",
      "/Users/alexandrerame/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
      "  DeprecationWarning)\n",
      "/Users/alexandrerame/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
      "  DeprecationWarning)\n",
      "/Users/alexandrerame/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
      "  DeprecationWarning)\n",
      "/Users/alexandrerame/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os.path\n",
    "import scipy\n",
    "import random\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble.base import BaseEnsemble\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import math\n",
    "\n",
    "from neuralnet import NeuralNet\n",
    "from load_new import getData\n",
    "\n",
    "\n",
    "\n",
    "class ModifiedXGBClassifier(xgb.XGBClassifier):\n",
    "    def __init__(self, max_depth=20, learning_rate=0.1, n_estimators=300, \n",
    "                 silent=True, objective='multi:softprob', max_features=0.3, subsample = 0.5):\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.silent = silent\n",
    "        self.n_estimators = n_estimators\n",
    "        self.objective = objective\n",
    "        self.max_features = max_features\n",
    "        self.subsample = subsample\n",
    "        self._Booster = xgb.Booster()\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return {'max_depth': self.max_depth,\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'n_estimators': self.n_estimators,\n",
    "                'silent': self.silent,\n",
    "                'objective': self.objective,\n",
    "                'max_features' : self.max_features,\n",
    "                'subsample' : self.subsample,\n",
    "                'num_class':4\n",
    "                }\n",
    "        \n",
    "    def get_xgb_params(self):\n",
    "        return {'eta': self.learning_rate,\n",
    "                'max_depth': self.max_depth,\n",
    "                'silent': 1 if self.silent else 0,\n",
    "                'objective': self.objective,\n",
    "                'bst:subsample': self.subsample,\n",
    "                'bst:colsample_bytree': self.max_features,\n",
    "                'num_class':4\n",
    "                }\n",
    "    \n",
    "class BlendedModel(BaseEnsemble):\n",
    "    def __init__(self, models=[], blending='average',nbFeatures=4):\n",
    "        self.models = models\n",
    "        self.blending = blending\n",
    "        self.logR = LogisticRegression(C=10)#,multi_class='multinomial',solver='lbfgs', max_iter=10000)\n",
    "        self.logRT= LogisticRegression(C=10)#,multi_class='multinomial',solver='lbfgs', max_iter=10000)\n",
    "        self.nn=NeuralNet(nbFeatures) \n",
    "        self.XGB=ModifiedXGBClassifier()\n",
    "        if self.blending not in ['average', 'most_confident']:\n",
    "            raise Exception('Wrong blending method')\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for model in self.models:\n",
    "            print 'Training model :'\n",
    "            print model.get_params()\n",
    "            model.fit(X, y)                \n",
    "        return self\n",
    "    \n",
    "    def fitLog(self,X,y,mod=0):\n",
    "        if mod==0:\n",
    "            preds = np.array(\n",
    "                        [model.predict_proba(X) for model in self.models]\n",
    "                    )\n",
    "            features=np.array([np.array([preds[j][i] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "            self.logR.fit(features, y)\n",
    "        elif mod==1:\n",
    "            preds = np.array(\n",
    "                        [model.predict_proba(X) for model in self.models]\n",
    "                    )\n",
    "            features=np.array([np.array([[math.log(preds[j][i][k]/(1-preds[j][i][k])) for k in range(4)] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "            self.logRT.fit(features, y)\n",
    "        return self\n",
    "    \n",
    "    def fitXGB(self,X,y):\n",
    "        preds = np.array(\n",
    "                        [model.predict_proba(X) for model in self.models]\n",
    "                    )\n",
    "        features=np.array([np.array([[math.log(preds[j][i][k]/(1-preds[j][i][k])) for k in range(4)] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "        #features= np.append(features, X, axis=1)\n",
    "        self.XGB.fit(features,y)\n",
    "        return self\n",
    "    \n",
    "    def predict_XGBproba(self,X):\n",
    "        preds = np.array(\n",
    "                        [model.predict_proba(X) for model in self.models]\n",
    "                    )\n",
    "        features=np.array([np.array([[math.log(preds[j][i][k]/(1-preds[j][i][k])) for k in range(4)] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "        #features= np.append(features, X, axis=1)\n",
    "        return self.XGB.predict_proba(features)\n",
    "    \n",
    "    def fitNN(self,X,y,lambda1=0.00000001,lambda2=0.00005,new=0,teX=[],teY=[],lr=0.001):\n",
    "        \n",
    "        preds = np.array(\n",
    "                        [model.predict_proba(X) for model in self.models]\n",
    "                    )\n",
    "        features=np.array([np.array([preds[j][i] for j in range(len(self.models))]).flatten() for i in range(len(X))]) \n",
    "        features= np.append(features, X, axis=1)\n",
    "        \n",
    "        if len(teX)>0:\n",
    "            preds = np.array(\n",
    "                        [model.predict_proba(teX) for model in self.models]\n",
    "                    )\n",
    "            featuresteX=np.array([np.array([preds[j][i] for j in range(len(self.models))]).flatten() for i in range(len(teX))])\n",
    "            featuresteX= np.append(featuresteX, teX, axis=1)\n",
    "        else:\n",
    "            featuresteX=[]\n",
    "            \n",
    "        self.nn.fit(features,y,lambda1,lambda2,new,featuresteX,teY,lr)\n",
    "        return self\n",
    "    \n",
    "    def predict_NNproba(self,X):\n",
    "        preds = np.array(\n",
    "                        [model.predict_proba(X) for model in self.models]\n",
    "                    )\n",
    "        features=np.array([np.array([preds[j][i] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "        features= np.append(features, X, axis=1)\n",
    "        return self.nn.predict_proba(features)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        preds = np.array(\n",
    "                    [model.predict_proba(X) for model in self.models]\n",
    "                )\n",
    "        if self.blending == 'average':\n",
    "            return np.mean(preds , axis=0 )\n",
    "        elif self.blending == 'most_confident':\n",
    "            def dirac_weights(entropies):\n",
    "                w = (entropies == np.min(entropies)).astype(float)\n",
    "                return w/np.sum(w)\n",
    "            def shannon_entropy(l):\n",
    "                l = [min(max(1e-5,p),1-1e-5) for p in l]\n",
    "                l = np.array(l)/sum(l)\n",
    "                return sum([-p*math.log(p) for p in l])\n",
    "            shannon_entropy_array = lambda l : np.apply_along_axis(shannon_entropy, 1, l)\n",
    "            entropies = np.array([shannon_entropy_array(pred) for pred in preds])\n",
    "            weights = np.apply_along_axis(dirac_weights, 0, entropies)\n",
    "            return np.sum(np.multiply(weights.T, preds.T).T, axis = 0)\n",
    "    \n",
    "    def predict_Logproba(self, X,mod=0):\n",
    "        if mod==0:\n",
    "            preds = np.array(\n",
    "                        [model.predict_proba(X) for model in self.models]\n",
    "                    )\n",
    "            features=np.array([np.array([preds[j][i] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "            preds=self.logR.predict_proba(features)\n",
    "            return preds\n",
    "        elif mod==1:\n",
    "            preds = np.array(\n",
    "                    [model.predict_proba(X) for model in self.models]\n",
    "                )\n",
    "            features=np.array([np.array([[math.log(preds[j][i][k]/(1-preds[j][i][k])) for k in range(4)] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "            preds=self.logRT.predict_proba(features)\n",
    "            return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    results = pd.io.pickle.read_pickle('../results/results_df_last.pkl')\n",
    "    models_to_use = results#[(results['best_score']<0.07) | ((results['best_score']<0.09)&(results['bst:eta']>0.3))]\n",
    "    models_to_use=models_to_use.sort(['best_score'])[:4]\n",
    "    models = []\n",
    "    for j,row in enumerate(models_to_use.iterrows()):\n",
    "            print j\n",
    "            hyperparams = dict(row[1])\n",
    "            models.append(ModifiedXGBClassifier(\n",
    "                                    max_depth=hyperparams['bst:max_depth'], \n",
    "                                    learning_rate=hyperparams['bst:eta'], \n",
    "                                    n_estimators=int(hyperparams['best_ntrees']),\n",
    "                                    max_features=hyperparams['bst:colsample_bytree'],\n",
    "                                    subsample=hyperparams['bst:subsample'],\n",
    "                                    silent=True, \n",
    "                                    objective='multi:softprob')\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_ntrees</th>\n",
       "      <th>best_score</th>\n",
       "      <th>bst:colsample_bytree</th>\n",
       "      <th>bst:eta</th>\n",
       "      <th>bst:max_depth</th>\n",
       "      <th>bst:subsample</th>\n",
       "      <th>early_stopping</th>\n",
       "      <th>n_round</th>\n",
       "      <th>ntrees</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 155</td>\n",
       "      <td> 0.060252</td>\n",
       "      <td> 0.793944</td>\n",
       "      <td> 0.107341</td>\n",
       "      <td> 12</td>\n",
       "      <td> 0.516188</td>\n",
       "      <td> 100</td>\n",
       "      <td> 800</td>\n",
       "      <td> 155</td>\n",
       "      <td> 0.060252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 164</td>\n",
       "      <td> 0.078468</td>\n",
       "      <td> 0.867487</td>\n",
       "      <td> 0.205979</td>\n",
       "      <td> 13</td>\n",
       "      <td> 0.534762</td>\n",
       "      <td> 100</td>\n",
       "      <td> 800</td>\n",
       "      <td> 164</td>\n",
       "      <td> 0.078468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   best_ntrees  best_score  bst:colsample_bytree   bst:eta  bst:max_depth  \\\n",
       "1          155    0.060252              0.793944  0.107341             12   \n",
       "0          164    0.078468              0.867487  0.205979             13   \n",
       "\n",
       "   bst:subsample  early_stopping  n_round  ntrees     score  \n",
       "1       0.516188             100      800     155  0.060252  \n",
       "0       0.534762             100      800     164  0.078468  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2224\n",
      "3106\n",
      "536\n",
      "759\n",
      "450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load_new.py:53: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  temp[:,j]=np.fft.fft(temp[:,j])\n"
     ]
    }
   ],
   "source": [
    "nb_fold=3\n",
    "dtrX,dtrY,dteX,dteY = getData(prop=0,oh=0)\n",
    "skf = StratifiedKFold(dtrY, nb_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':  \n",
    "    \n",
    "    rNNTe=np.zeros(nb_fold)\n",
    "    rNNVa=np.zeros(nb_fold)\n",
    "\n",
    "        \n",
    "    for j,(train, test) in enumerate(skf):\n",
    "        if j==0:\n",
    "        #print j,train, test\n",
    "            vaX=dtrX[test,][:(len(test)/2),]\n",
    "            teX=dtrX[test,][(len(test)/2):,]\n",
    "            trX=dtrX[train,]\n",
    "            vaY=dtrY[test][:(len(test)/2)]\n",
    "            teY=dtrY[test][(len(test)/2):]\n",
    "            trY=dtrY[train,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.,  3.,  3., ...,  1.,  3.,  1.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model :\n",
      "{'n_estimators': 155, 'max_features': 0.79394382111062678, 'num_class': 4, 'silent': True, 'subsample': 0.51618845140831149, 'objective': 'multi:softprob', 'learning_rate': 0.1073408314176849, 'max_depth': 12.0}\n",
      "Training model :\n",
      "{'n_estimators': 164, 'max_features': 0.86748731814423963, 'num_class': 4, 'silent': True, 'subsample': 0.53476195999554665, 'objective': 'multi:softprob', 'learning_rate': 0.20597855941231538, 'max_depth': 13.0}\n",
      "blend nn\n",
      "200 5 150 0.5 0.5\n",
      "1071\n",
      "10 5 0.452847805789 1.38629206081\n",
      "1.38701252971 0.000584520868458 0.000141709036303 1169.04173692 14.1709036303\n",
      "scoreTe 0.483660130719\n",
      "20 5 0.452847805789 0.998596811988\n",
      "1.09502747391 0.000737488244802 0.000235910242695 1474.9764896 23.5910242695\n",
      "scoreTe 0.483660130719\n",
      "30 5 0.845004668534 0.531625847481\n",
      "0.684364176166 0.00134057606516 0.00093536603392 2681.15213032 93.536603392\n",
      "scoreTe 0.830065359477\n",
      "40 5 0.859943977591 0.370963238616\n",
      "0.324691733935 0.00181840164366 0.00173956836544 3636.80328732 173.956836544\n",
      "scoreTe 0.851540616246\n",
      "50 5 0.869281045752 0.342270806928\n",
      "0.669163048521 0.00213933544996 0.00247225650968 4278.67089993 247.225650968\n",
      "scoreTe 0.868347338936\n",
      "60 5 0.874883286648 0.324859882731\n",
      "0.208482191858 0.00242213228482 0.00319856628493 4844.26456963 319.856628493\n",
      "scoreTe 0.869281045752\n",
      "70 5 0.874883286648 0.308459359587\n",
      "0.440153940621 0.00266934060445 0.00391622174045 5338.68120891 391.622174045\n",
      "scoreTe 0.865546218487\n",
      "80 5 0.929038281979 0.243073798921\n",
      "0.0926969403344 0.00294036413389 0.00469765948132 5880.72826777 469.765948132\n",
      "scoreTe 0.935574229692\n",
      "90 5 0.937441643324 0.240928838061\n",
      "0.688955535145 0.00316028646413 0.00544081879154 6320.57292826 544.081879154\n",
      "scoreTe 0.935574229692\n",
      "100 5 0.935574229692 0.214885181631\n",
      "2.10765078312 0.00335494399572 0.00615361380314 6709.88799143 615.361380314\n",
      "scoreTe 0.93837535014\n",
      "110 5 0.942110177404 0.215498855591\n",
      "0.297606479223 0.00352693334052 0.00683226213829 7053.86668103 683.226213829\n",
      "scoreTe 0.936507936508\n",
      "120 5 0.936507936508 0.204472895866\n",
      "1.15861372502 0.00368961743949 0.00752664350663 7379.23487897 752.664350663\n",
      "scoreTe 0.935574229692\n",
      "130 5 0.941176470588 0.205261807529\n",
      "0.463314762424 0.00383150829361 0.00820919314587 7663.01658723 820.919314587\n",
      "scoreTe 0.931839402428\n",
      "140 5 0.937441643324 0.19176817963\n",
      "0.133099200994 0.00396679872375 0.00889438732578 7933.5974475 889.438732578\n",
      "scoreTe 0.939309056956\n",
      "150 5 0.939309056956 0.201003371524\n",
      "0.199565138499 0.00409267784447 0.00956356868391 8185.35568895 956.356868391\n",
      "scoreTe 0.939309056956\n",
      "160 5 0.947712418301 0.18978762252\n",
      "0.412214268225 0.00421931596632 0.010266008303 8438.63193264 1026.6008303\n",
      "scoreTe 0.942110177404\n",
      "170 5 0.947712418301 0.181761556501\n",
      "0.0440671990017 0.00434499142849 0.0109725637187 8689.98285698 1097.25637187\n",
      "scoreTe 0.944911297852\n",
      "180 5 0.947712418301 0.173749229262\n",
      "0.134666900172 0.00445514772414 0.0116259828712 8910.29544828 1162.59828712\n",
      "scoreTe 0.93837535014\n",
      "190 5 0.948646125117 0.19416285997\n",
      "0.369392059044 0.00454754653953 0.0122273795112 9095.09307905 1222.73795112\n",
      "scoreTe 0.941176470588\n",
      "200 5 0.946778711485 0.181507422733\n",
      "0.224960738687 0.00464262062449 0.0128311737242 9285.24124897 1283.11737242\n",
      "scoreTe 0.93837535014\n",
      "score blend nn\n",
      "0.93837535014\n",
      "score blend nn val set\n",
      "0.946778711485\n",
      "rNNTe\n",
      "[ 0.93837535  0.          0.        ]\n",
      "rNNVa\n",
      "[ 0.94677871  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  \n",
    "    \n",
    "    rNNTe=np.zeros(nb_fold)\n",
    "    rNNVa=np.zeros(nb_fold)\n",
    "\n",
    "        \n",
    "    for j,(train, test) in enumerate(skf):\n",
    "        if j==0:\n",
    "        #print j,train, test\n",
    "            vaX=dtrX[test,][:(len(test)/2),]\n",
    "            teX=dtrX[test,][(len(test)/2):,]\n",
    "            trX=dtrX[train,]\n",
    "            vaY=dtrY[test,][:(len(test)/2),]\n",
    "            teY=dtrY[test,][(len(test)/2):,]\n",
    "            trY=dtrY[train,]\n",
    "\n",
    "\n",
    "            bmdl = BlendedModel(models,nbFeatures=(len(models)*4+len(trX[0])))\n",
    "            bmdl.fit(trX,trY)\n",
    "            \n",
    "            #for lambda1,lambda2 in zip([\n",
    "            print(\"blend nn\")\n",
    "            bmdl.fitNN(vaX,vaY,lambda1=0.0000005,lambda2=0.00001,teX=teX,teY=teY)\n",
    "\n",
    "            predictTe=bmdl.predict_NNproba(teX)\n",
    "            scoreTe=np.mean(np.argmax(predictTe, axis=1) == teY) \n",
    "            print(\"score blend nn\")\n",
    "            print(scoreTe)\n",
    "            rNNTe[j]=scoreTe\n",
    "\n",
    "            predictVa=bmdl.predict_NNproba(vaX)\n",
    "            scoreVa=np.mean(np.argmax(predictVa, axis=1) == vaY) \n",
    "            print(\"score blend nn val set\")\n",
    "            print(scoreVa)\n",
    "            rNNVa[j]=scoreVa\n",
    "\n",
    "    print \"rNNTe\"\n",
    "    print rNNTe\n",
    "    print \"rNNVa\"\n",
    "    print rNNVa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score models\n",
      "0.945895522388\n",
      "0.946828358209\n",
      "0.944962686567\n",
      "0.943097014925\n"
     ]
    }
   ],
   "source": [
    "print(\"score models\")\n",
    "for m,model in enumerate(models):\n",
    "    print(np.mean(np.argmax(model.predict_proba(teX), axis=1) == teY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score average of models\n",
      "0.948694029851\n",
      "blend logistic regression\n",
      "score blend logistic regression\n",
      "0.945895522388\n",
      "score blend logistic regression val test\n",
      "0.936507936508\n",
      "blend logistic regression with log(x/(1-x)\n",
      "score blend logistic regression with log(x/(1-x)\n",
      "0.944029850746\n",
      "score blend logistic regression with log(x/(1-x) val test\n",
      "0.940242763772\n"
     ]
    }
   ],
   "source": [
    "predictTe=bmdl.predict_proba(teX)\n",
    "scoreTe=np.mean(np.argmax(predictTe, axis=1) == teY) \n",
    "print(\"score average of models\")\n",
    "print(scoreTe)\n",
    "\n",
    "print(\"blend logistic regression\")\n",
    "bmdl.fitLog(vaX,vaY)\n",
    "\n",
    "predictTe=bmdl.predict_Logproba(teX)\n",
    "scoreTe=np.mean(np.argmax(predictTe, axis=1) == teY) \n",
    "print(\"score blend logistic regression\")\n",
    "print(scoreTe)\n",
    "\n",
    "\n",
    "predictVa=bmdl.predict_Logproba(vaX,mod=0)\n",
    "scoreVa=np.mean(np.argmax(predictVa, axis=1) == vaY) \n",
    "print(\"score blend logistic regression val test\")\n",
    "print(scoreVa)\n",
    "       \n",
    "           \n",
    "print(\"blend logistic regression with log(x/(1-x)\")\n",
    "bmdl.fitLog(vaX,vaY,mod=1)\n",
    "predictTe=bmdl.predict_Logproba(teX,mod=1)\n",
    "scoreTe=np.mean(np.argmax(predictTe, axis=1) == teY) \n",
    "print(\"score blend logistic regression with log(x/(1-x)\")\n",
    "print(scoreTe)\n",
    "\n",
    "\n",
    "predictVa=bmdl.predict_Logproba(vaX,mod=1)\n",
    "scoreVa=np.mean(np.argmax(predictVa, axis=1) == vaY) \n",
    "print(\"score blend logistic regression with log(x/(1-x) val test\")\n",
    "print(scoreVa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blend nn\n",
      "200 5 150 0.2 0.4\n",
      "1071\n",
      "10 5 0.988795518207 0.0247885748217\n",
      "0.012380550439 0.0 0.0 11110.7001978 2409.5518776\n",
      "scoreTe 0.939365671642\n",
      "20 5 0.986928104575 0.0295253385682\n",
      "0.0127036667653 0.0 0.0 11329.167318 2509.24930597\n",
      "scoreTe 0.9375\n",
      "30 5 0.990662931839 0.0198829996661\n",
      "0.00408293510102 0.0 0.0 11508.3268008 2599.15338832\n",
      "scoreTe 0.940298507463\n",
      "40 5 0.992530345472 0.0175836409914\n",
      "0.000170022627524 0.0 0.0 11730.0218639 2702.25989392\n",
      "scoreTe 0.9375\n",
      "50 5 0.987861811391 0.0221001606801\n",
      "0.00186877830533 0.0 0.0 11925.0541316 2800.21353591\n",
      "scoreTe 0.944029850746\n",
      "60 5 0.988795518207 0.0263464115172\n",
      "0.00801669951797 0.0 0.0 12100.1706556 2892.31205918\n",
      "scoreTe 0.942164179104\n",
      "70 5 0.992530345472 0.0229496714865\n",
      "0.0295303606575 0.0 0.0 12292.1676453 2995.78294325\n",
      "scoreTe 0.944029850746\n",
      "80 5 0.989729225023 0.0220802476201\n",
      "0.0012865615144 0.0 0.0 12468.138041 3089.88324609\n",
      "scoreTe 0.945895522388\n",
      "90 5 0.993464052288 0.0192367216913\n",
      "0.0349641011283 0.0 0.0 12617.4636858 3182.80255842\n",
      "scoreTe 0.943097014925\n",
      "100 5 0.993464052288 0.0203930442731\n",
      "0.000345409452651 0.0 0.0 12801.235756 3284.21508228\n",
      "scoreTe 0.938432835821\n",
      "110 5 0.992530345472 0.0175885397585\n",
      "0.00985626408806 0.0 0.0 12953.4247532 3367.31368279\n",
      "scoreTe 0.9375\n",
      "120 5 0.993464052288 0.0197437637676\n",
      "1.42875064688e-05 0.0 0.0 13097.3538471 3454.495873\n",
      "scoreTe 0.943097014925\n",
      "130 5 0.993464052288 0.016540270068\n",
      "0.000528550105231 0.0 0.0 13233.7273768 3544.28922886\n",
      "scoreTe 0.944029850746\n",
      "140 5 0.991596638655 0.0201325300255\n",
      "0.00466399831826 0.0 0.0 13403.3204156 3645.58549398\n",
      "scoreTe 0.930970149254\n",
      "150 5 0.993464052288 0.0193208210217\n",
      "0.044858541249 0.0 0.0 13572.7069291 3748.0444912\n",
      "scoreTe 0.938432835821\n",
      "160 5 0.993464052288 0.0173947441818\n",
      "1.91701820748e-05 0.0 0.0 13715.3864711 3840.39787225\n",
      "scoreTe 0.939365671642\n",
      "170 5 0.993464052288 0.0161185998338\n",
      "0.00731243468564 0.0 0.0 13835.3314428 3923.48713637\n",
      "scoreTe 0.941231343284\n",
      "180 5 0.990662931839 0.0212077970008\n",
      "0.00287736406312 0.0 0.0 13981.9210026 4019.0183898\n",
      "scoreTe 0.944029850746\n",
      "190 5 0.994397759104 0.0150269738564\n",
      "0.00457354450946 0.0 0.0 14119.1493336 4107.28659376\n",
      "scoreTe 0.938432835821\n",
      "200 5 0.99533146592 0.0135465495641\n",
      "0.00351420638021 0.0 0.0 14268.622233 4204.2029044\n",
      "scoreTe 0.944962686567\n",
      "score blend nn\n",
      "0.944962686567\n",
      "score blend nn val set\n",
      "0.99533146592\n",
      "rNNTe\n",
      "[ 0.93843284  0.          0.94496269]\n",
      "rNNVa\n",
      "[ 0.97759104  0.          0.99533147]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  \n",
    "          \n",
    "    print(\"blend nn\")\n",
    "    bmdl.fitNN(vaX,vaY,lambda1=0.000000,lambda2=0.0000,teX=teX,teY=teY)\n",
    "\n",
    "    predictTe=bmdl.predict_NNproba(teX)\n",
    "    scoreTe=np.mean(np.argmax(predictTe, axis=1) == teY) \n",
    "    print(\"score blend nn\")\n",
    "    print(scoreTe)\n",
    "    rNNTe[j]=scoreTe\n",
    "\n",
    "    predictVa=bmdl.predict_NNproba(vaX)\n",
    "    scoreVa=np.mean(np.argmax(predictVa, axis=1) == vaY) \n",
    "    print(\"score blend nn val set\")\n",
    "    print(scoreVa)\n",
    "    rNNVa[j]=scoreVa\n",
    "\n",
    "    print \"rNNTe\"\n",
    "    print rNNTe\n",
    "    print \"rNNVa\"\n",
    "    print rNNVa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -1.,  2.,  0.,  0.,  0., -1., -1., -1.,  2.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((teY-1) == (teY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
