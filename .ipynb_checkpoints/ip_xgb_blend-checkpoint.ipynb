{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os.path\n",
    "import scipy\n",
    "import random\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble.base import BaseEnsemble\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import math\n",
    "\n",
    "from neuralnet import NeuralNet\n",
    "from load_new import getData\n",
    "\n",
    "\n",
    "\n",
    "class ModifiedXGBClassifier(xgb.XGBClassifier):\n",
    "    def __init__(self, max_depth=20, learning_rate=0.1, n_estimators=300, \n",
    "                 silent=True, objective='multi:softprob', max_features=0.3, subsample = 0.5):\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.silent = silent\n",
    "        self.n_estimators = n_estimators\n",
    "        self.objective = objective\n",
    "        self.max_features = max_features\n",
    "        self.subsample = subsample\n",
    "        self._Booster = xgb.Booster()\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return {'max_depth': self.max_depth,\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'n_estimators': self.n_estimators,\n",
    "                'silent': self.silent,\n",
    "                'objective': self.objective,\n",
    "                'max_features' : self.max_features,\n",
    "                'subsample' : self.subsample,\n",
    "                'num_class':4\n",
    "                }\n",
    "        \n",
    "    def get_xgb_params(self):\n",
    "        return {'eta': self.learning_rate,\n",
    "                'max_depth': self.max_depth,\n",
    "                'silent': 1 if self.silent else 0,\n",
    "                'objective': self.objective,\n",
    "                'bst:subsample': self.subsample,\n",
    "                'bst:colsample_bytree': self.max_features,\n",
    "                'num_class':4\n",
    "                }\n",
    "    \n",
    "class BlendedModel(BaseEnsemble):\n",
    "    def __init__(self, models=[], blending='average',nbFeatures=4):\n",
    "        self.models = models\n",
    "        self.blending = blending\n",
    "        self.logR = LogisticRegression(C=10)#,multi_class='multinomial',solver='lbfgs', max_iter=10000)\n",
    "        self.logRT= LogisticRegression(C=10)#,multi_class='multinomial',solver='lbfgs', max_iter=10000)\n",
    "        self.nn=NeuralNet(nbFeatures) \n",
    "        self.XGB=ModifiedXGBClassifier()\n",
    "        if self.blending not in ['average', 'most_confident']:\n",
    "            raise Exception('Wrong blending method')\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for model in self.models:\n",
    "            print 'Training model :'\n",
    "            print model.get_params()\n",
    "            model.fit(X, y)                \n",
    "        return self\n",
    "    \n",
    "    def fitLog(self,X,y,mod=0):\n",
    "        if mod==0:\n",
    "            preds = np.array(\n",
    "                        [model.predict_proba(X) for model in self.models]\n",
    "                    )\n",
    "            features=np.array([np.array([preds[j][i] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "            self.logR.fit(features, y)\n",
    "        elif mod==1:\n",
    "            preds = np.array(\n",
    "                        [model.predict_proba(X) for model in self.models]\n",
    "                    )\n",
    "            features=np.array([np.array([[math.log(preds[j][i][k]/(1-preds[j][i][k])) for k in range(4)] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "            self.logRT.fit(features, y)\n",
    "        return self\n",
    "    \n",
    "    def fitXGB(self,X,y):\n",
    "        preds = np.array(\n",
    "                        [model.predict_proba(X) for model in self.models]\n",
    "                    )\n",
    "        features=np.array([np.array([[math.log(preds[j][i][k]/(1-preds[j][i][k])) for k in range(4)] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "        #features= np.append(features, X, axis=1)\n",
    "        self.XGB.fit(features,y)\n",
    "        return self\n",
    "    \n",
    "    def predict_XGBproba(self,X):\n",
    "        preds = np.array(\n",
    "                        [model.predict_proba(X) for model in self.models]\n",
    "                    )\n",
    "        features=np.array([np.array([[math.log(preds[j][i][k]/(1-preds[j][i][k])) for k in range(4)] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "        #features= np.append(features, X, axis=1)\n",
    "        return self.XGB.predict_proba(features)\n",
    "    \n",
    "    def fitNN(self,X,y,lambda1=0.00000001,lambda2=0.00005,new=0,teX=[],teY=[],lr=0.001):\n",
    "        \n",
    "        preds = np.array(\n",
    "                        [model.predict_proba(X) for model in self.models]\n",
    "                    )\n",
    "        features=np.array([np.array([preds[j][i] for j in range(len(self.models))]).flatten() for i in range(len(X))]) \n",
    "        features= np.append(features, X, axis=1)\n",
    "        \n",
    "        if len(teX)>0:\n",
    "            preds = np.array(\n",
    "                        [model.predict_proba(teX) for model in self.models]\n",
    "                    )\n",
    "            featuresteX=np.array([np.array([preds[j][i] for j in range(len(self.models))]).flatten() for i in range(len(teX))])\n",
    "            featuresteX= np.append(featuresteX, teX, axis=1)\n",
    "        else:\n",
    "            featuresteX=[]\n",
    "            \n",
    "        self.nn.fit(features,y,lambda1,lambda2,new,featuresteX,teY,lr)\n",
    "        return self\n",
    "    \n",
    "    def predict_NNproba(self,X):\n",
    "        preds = np.array(\n",
    "                        [model.predict_proba(X) for model in self.models]\n",
    "                    )\n",
    "        features=np.array([np.array([preds[j][i] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "        features= np.append(features, X, axis=1)\n",
    "        return self.nn.predict_proba(features)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        preds = np.array(\n",
    "                    [model.predict_proba(X) for model in self.models]\n",
    "                )\n",
    "        if self.blending == 'average':\n",
    "            return np.mean(preds , axis=0 )\n",
    "        elif self.blending == 'most_confident':\n",
    "            def dirac_weights(entropies):\n",
    "                w = (entropies == np.min(entropies)).astype(float)\n",
    "                return w/np.sum(w)\n",
    "            def shannon_entropy(l):\n",
    "                l = [min(max(1e-5,p),1-1e-5) for p in l]\n",
    "                l = np.array(l)/sum(l)\n",
    "                return sum([-p*math.log(p) for p in l])\n",
    "            shannon_entropy_array = lambda l : np.apply_along_axis(shannon_entropy, 1, l)\n",
    "            entropies = np.array([shannon_entropy_array(pred) for pred in preds])\n",
    "            weights = np.apply_along_axis(dirac_weights, 0, entropies)\n",
    "            return np.sum(np.multiply(weights.T, preds.T).T, axis = 0)\n",
    "    \n",
    "    def predict_Logproba(self, X,mod=0):\n",
    "        if mod==0:\n",
    "            preds = np.array(\n",
    "                        [model.predict_proba(X) for model in self.models]\n",
    "                    )\n",
    "            features=np.array([np.array([preds[j][i] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "            preds=self.logR.predict_proba(features)\n",
    "            return preds\n",
    "        elif mod==1:\n",
    "            preds = np.array(\n",
    "                    [model.predict_proba(X) for model in self.models]\n",
    "                )\n",
    "            features=np.array([np.array([[math.log(preds[j][i][k]/(1-preds[j][i][k])) for k in range(4)] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "            preds=self.logRT.predict_proba(features)\n",
    "            return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    results = pd.io.pickle.read_pickle('results/results_df_last.pkl')\n",
    "    models_to_use = results[(results['best_score']<0.07) | ((results['best_score']<0.09)&(results['bst:eta']>0.3))]\n",
    "    models_to_use=models_to_use.sort(['best_score'])[:4]\n",
    "    models = []\n",
    "    for j,row in enumerate(models_to_use.iterrows()):\n",
    "            print j\n",
    "            hyperparams = dict(row[1])\n",
    "            models.append(ModifiedXGBClassifier(\n",
    "                                    max_depth=hyperparams['bst:max_depth'], \n",
    "                                    learning_rate=hyperparams['bst:eta'], \n",
    "                                    n_estimators=int(hyperparams['best_ntrees']),\n",
    "                                    max_features=hyperparams['bst:colsample_bytree'],\n",
    "                                    subsample=hyperparams['bst:subsample'],\n",
    "                                    silent=True, \n",
    "                                    objective='multi:softprob')\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2224\n",
      "3106\n",
      "536\n",
      "759\n",
      "450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load_new.py:53: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  temp[:,j]=np.fft.fft(temp[:,j])\n"
     ]
    }
   ],
   "source": [
    "nb_fold=3\n",
    "dtrX,dtrY,dteX,dteY = getData(prop=0,oh=0)\n",
    "skf = StratifiedKFold(dtrY, nb_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model :\n",
      "{'n_estimators': 145, 'max_features': 0.87143152648660338, 'num_class': 4, 'silent': True, 'subsample': 0.5677004279168405, 'objective': 'multi:softprob', 'learning_rate': 0.075351999024363861, 'max_depth': 17.0}\n",
      "Training model :\n",
      "{'n_estimators': 77, 'max_features': 0.81208414906970527, 'num_class': 4, 'silent': True, 'subsample': 0.53454467116594329, 'objective': 'multi:softprob', 'learning_rate': 0.19966463026760631, 'max_depth': 28.0}\n",
      "Training model :\n",
      "{'n_estimators': 79, 'max_features': 0.97500489683440184, 'num_class': 4, 'silent': True, 'subsample': 0.55386963110903675, 'objective': 'multi:softprob', 'learning_rate': 0.13222069369172015, 'max_depth': 22.0}\n",
      "Training model :\n",
      "{'n_estimators': 111, 'max_features': 0.82207855775474548, 'num_class': 4, 'silent': True, 'subsample': 0.74990595153698836, 'objective': 'multi:softprob', 'learning_rate': 0.087168586238034751, 'max_depth': 26.0}\n",
      "blend nn\n",
      "200 5 150 0.2 0.4\n",
      "1071\n",
      "10 5 0.463118580766 1.38628845143\n",
      "1.38701456778 0.000586458944499 0.000141558489521 1172.917889 14.1558489521\n",
      "scoreTe 0.472947761194\n",
      "20 5 0.355742296919 0.776256211061\n",
      "0.474495420652 0.001038652882 0.000676950495235 2077.30576401 67.6950495235\n",
      "scoreTe 0.32276119403\n",
      "30 5 0.919701213819 0.324709683414\n",
      "0.107921970926 0.00162981147832 0.00154932439527 3259.62295664 154.932439527\n",
      "scoreTe 0.921641791045\n",
      "40 5 0.942110177404 0.216965757454\n",
      "0.0782564620722 0.00202447951732 0.00234971173728 4048.95903465 234.971173728\n",
      "scoreTe 0.94776119403\n",
      "50 5 0.944911297852 0.209652261518\n",
      "0.135322867187 0.00230414974289 0.00306717373682 4608.29948578 306.717373682\n",
      "scoreTe 0.946828358209\n",
      "60 5 0.956115779645 0.210260415977\n",
      "0.154480684455 0.00254933972791 0.00379788901063 5098.67945581 379.788901063\n",
      "scoreTe 0.944029850746\n",
      "70 5 0.948646125117 0.175506979572\n",
      "0.0738821132008 0.00276766456679 0.00454390563243 5535.32913358 454.390563243\n",
      "scoreTe 0.946828358209\n",
      "80 5 0.956115779645 0.146243744055\n",
      "0.0511942217874 0.00296559068898 0.00529594123356 5931.18137796 529.594123356\n",
      "scoreTe 0.94776119403\n",
      "90 5 0.961718020542 0.120943091328\n",
      "0.138045547933 0.00313508449655 0.00603642878618 6270.16899309 603.642878618\n",
      "scoreTe 0.94776119403\n",
      "100 5 0.963585434174 0.118931831082\n",
      "0.078817012457 0.00330077785782 0.00677036612646 6601.55571564 677.036612646\n",
      "scoreTe 0.941231343284\n",
      "110 5 0.963585434174 0.117957937243\n",
      "0.0580638508752 0.00345083417022 0.00750654725251 6901.66834044 750.654725251\n",
      "scoreTe 0.946828358209\n",
      "120 5 0.970121381886 0.0977841768508\n",
      "0.114882648276 0.0035889244279 0.00820002566262 7177.8488558 820.002566262\n",
      "scoreTe 0.941231343284\n",
      "130 5 0.972922502334 0.0771283010054\n",
      "0.0678182693535 0.0037185558489 0.00888909870027 7437.1116978 888.909870027\n",
      "scoreTe 0.944029850746\n",
      "140 5 0.971055088702 0.091635151871\n",
      "0.0356122039879 0.00383823002359 0.00954612236513 7676.46004719 954.612236513\n",
      "scoreTe 0.942164179104\n",
      "150 5 0.974789915966 0.0942212431118\n",
      "0.0441725230555 0.00395688267807 0.0102774150685 7913.76535614 1027.74150685\n",
      "scoreTe 0.9375\n",
      "160 5 0.976657329599 0.0865186870335\n",
      "0.0287357123911 0.00406013141562 0.0109327171292 8120.26283124 1093.27171292\n",
      "scoreTe 0.941231343284\n",
      "170 5 0.971988795518 0.0869991925888\n",
      "0.0408140529424 0.0041663661657 0.0116262636474 8332.73233139 1162.62636474\n",
      "scoreTe 0.9375\n",
      "180 5 0.981325863679 0.0590379431197\n",
      "0.261697332726 0.00426209305165 0.0122848294524 8524.18610331 1228.48294524\n",
      "scoreTe 0.942164179104\n",
      "190 5 0.980392156863 0.0737512928243\n",
      "0.129570664661 0.00433348111316 0.0128150777603 8666.96222633 1281.50777603\n",
      "scoreTe 0.935634328358\n",
      "200 5 0.977591036415 0.0594294254113\n",
      "0.0452963711121 0.00442035583025 0.0134460142302 8840.71166051 1344.60142302\n",
      "scoreTe 0.938432835821\n",
      "score blend nn\n",
      "0.938432835821\n",
      "score blend nn val set\n",
      "0.977591036415\n",
      "rNNTe\n",
      "[ 0.93843284  0.          0.        ]\n",
      "rNNVa\n",
      "[ 0.97759104  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  \n",
    "    \n",
    "    rNNTe=np.zeros(nb_fold)\n",
    "    rNNVa=np.zeros(nb_fold)\n",
    "\n",
    "        \n",
    "    for j,(train, test) in enumerate(skf):\n",
    "        if j==0:\n",
    "        #print j,train, test\n",
    "            vaX=dtrX[test,][:(len(test)/2),]\n",
    "            teX=dtrX[test,][(len(test)/2):,]\n",
    "            trX=dtrX[train,]\n",
    "            vaY=dtrY[test,][:(len(test)/2),]\n",
    "            teY=dtrY[test,][(len(test)/2):,]\n",
    "            trY=dtrY[train,]\n",
    "\n",
    "\n",
    "            bmdl = BlendedModel(models,nbFeatures=(len(models)*4+len(trX[0])))\n",
    "            bmdl.fit(trX,trY)\n",
    "            \n",
    "            #for lambda1,lambda2 in zip([\n",
    "            print(\"blend nn\")\n",
    "            bmdl.fitNN(vaX,vaY,lambda1=0.0000005,lambda2=0.00001,teX=teX,teY=teY)\n",
    "\n",
    "            predictTe=bmdl.predict_NNproba(teX)\n",
    "            scoreTe=np.mean(np.argmax(predictTe, axis=1) == teY) \n",
    "            print(\"score blend nn\")\n",
    "            print(scoreTe)\n",
    "            rNNTe[j]=scoreTe\n",
    "\n",
    "            predictVa=bmdl.predict_NNproba(vaX)\n",
    "            scoreVa=np.mean(np.argmax(predictVa, axis=1) == vaY) \n",
    "            print(\"score blend nn val set\")\n",
    "            print(scoreVa)\n",
    "            rNNVa[j]=scoreVa\n",
    "\n",
    "    print \"rNNTe\"\n",
    "    print rNNTe\n",
    "    print \"rNNVa\"\n",
    "    print rNNVa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score models\n",
      "0.945895522388\n",
      "0.946828358209\n",
      "0.944962686567\n",
      "0.943097014925\n"
     ]
    }
   ],
   "source": [
    "print(\"score models\")\n",
    "for m,model in enumerate(models):\n",
    "    print(np.mean(np.argmax(model.predict_proba(teX), axis=1) == teY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score average of models\n",
      "0.948694029851\n",
      "blend logistic regression\n",
      "score blend logistic regression\n",
      "0.945895522388\n",
      "score blend logistic regression val test\n",
      "0.936507936508\n",
      "blend logistic regression with log(x/(1-x)\n",
      "score blend logistic regression with log(x/(1-x)\n",
      "0.944029850746\n",
      "score blend logistic regression with log(x/(1-x) val test\n",
      "0.940242763772\n"
     ]
    }
   ],
   "source": [
    "predictTe=bmdl.predict_proba(teX)\n",
    "scoreTe=np.mean(np.argmax(predictTe, axis=1) == teY) \n",
    "print(\"score average of models\")\n",
    "print(scoreTe)\n",
    "\n",
    "print(\"blend logistic regression\")\n",
    "bmdl.fitLog(vaX,vaY)\n",
    "\n",
    "predictTe=bmdl.predict_Logproba(teX)\n",
    "scoreTe=np.mean(np.argmax(predictTe, axis=1) == teY) \n",
    "print(\"score blend logistic regression\")\n",
    "print(scoreTe)\n",
    "\n",
    "\n",
    "predictVa=bmdl.predict_Logproba(vaX,mod=0)\n",
    "scoreVa=np.mean(np.argmax(predictVa, axis=1) == vaY) \n",
    "print(\"score blend logistic regression val test\")\n",
    "print(scoreVa)\n",
    "       \n",
    "           \n",
    "print(\"blend logistic regression with log(x/(1-x)\")\n",
    "bmdl.fitLog(vaX,vaY,mod=1)\n",
    "predictTe=bmdl.predict_Logproba(teX,mod=1)\n",
    "scoreTe=np.mean(np.argmax(predictTe, axis=1) == teY) \n",
    "print(\"score blend logistic regression with log(x/(1-x)\")\n",
    "print(scoreTe)\n",
    "\n",
    "\n",
    "predictVa=bmdl.predict_Logproba(vaX,mod=1)\n",
    "scoreVa=np.mean(np.argmax(predictVa, axis=1) == vaY) \n",
    "print(\"score blend logistic regression with log(x/(1-x) val test\")\n",
    "print(scoreVa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blend nn\n",
      "200 5 150 0.2 0.4\n",
      "1071\n",
      "10 5 0.988795518207 0.0247885748217\n",
      "0.012380550439 0.0 0.0 11110.7001978 2409.5518776\n",
      "scoreTe 0.939365671642\n",
      "20 5 0.986928104575 0.0295253385682\n",
      "0.0127036667653 0.0 0.0 11329.167318 2509.24930597\n",
      "scoreTe 0.9375\n",
      "30 5 0.990662931839 0.0198829996661\n",
      "0.00408293510102 0.0 0.0 11508.3268008 2599.15338832\n",
      "scoreTe 0.940298507463\n",
      "40 5 0.992530345472 0.0175836409914\n",
      "0.000170022627524 0.0 0.0 11730.0218639 2702.25989392\n",
      "scoreTe 0.9375\n",
      "50 5 0.987861811391 0.0221001606801\n",
      "0.00186877830533 0.0 0.0 11925.0541316 2800.21353591\n",
      "scoreTe 0.944029850746\n",
      "60 5 0.988795518207 0.0263464115172\n",
      "0.00801669951797 0.0 0.0 12100.1706556 2892.31205918\n",
      "scoreTe 0.942164179104\n",
      "70 5 0.992530345472 0.0229496714865\n",
      "0.0295303606575 0.0 0.0 12292.1676453 2995.78294325\n",
      "scoreTe 0.944029850746\n",
      "80 5 0.989729225023 0.0220802476201\n",
      "0.0012865615144 0.0 0.0 12468.138041 3089.88324609\n",
      "scoreTe 0.945895522388\n",
      "90 5 0.993464052288 0.0192367216913\n",
      "0.0349641011283 0.0 0.0 12617.4636858 3182.80255842\n",
      "scoreTe 0.943097014925\n",
      "100 5 0.993464052288 0.0203930442731\n",
      "0.000345409452651 0.0 0.0 12801.235756 3284.21508228\n",
      "scoreTe 0.938432835821\n",
      "110 5 0.992530345472 0.0175885397585\n",
      "0.00985626408806 0.0 0.0 12953.4247532 3367.31368279\n",
      "scoreTe 0.9375\n",
      "120 5 0.993464052288 0.0197437637676\n",
      "1.42875064688e-05 0.0 0.0 13097.3538471 3454.495873\n",
      "scoreTe 0.943097014925\n",
      "130 5 0.993464052288 0.016540270068\n",
      "0.000528550105231 0.0 0.0 13233.7273768 3544.28922886\n",
      "scoreTe 0.944029850746\n",
      "140 5 0.991596638655 0.0201325300255\n",
      "0.00466399831826 0.0 0.0 13403.3204156 3645.58549398\n",
      "scoreTe 0.930970149254\n",
      "150 5 0.993464052288 0.0193208210217\n",
      "0.044858541249 0.0 0.0 13572.7069291 3748.0444912\n",
      "scoreTe 0.938432835821\n",
      "160 5 0.993464052288 0.0173947441818\n",
      "1.91701820748e-05 0.0 0.0 13715.3864711 3840.39787225\n",
      "scoreTe 0.939365671642\n",
      "170 5 0.993464052288 0.0161185998338\n",
      "0.00731243468564 0.0 0.0 13835.3314428 3923.48713637\n",
      "scoreTe 0.941231343284\n",
      "180 5 0.990662931839 0.0212077970008\n",
      "0.00287736406312 0.0 0.0 13981.9210026 4019.0183898\n",
      "scoreTe 0.944029850746\n",
      "190 5 0.994397759104 0.0150269738564\n",
      "0.00457354450946 0.0 0.0 14119.1493336 4107.28659376\n",
      "scoreTe 0.938432835821\n",
      "200 5 0.99533146592 0.0135465495641\n",
      "0.00351420638021 0.0 0.0 14268.622233 4204.2029044\n",
      "scoreTe 0.944962686567\n",
      "score blend nn\n",
      "0.944962686567\n",
      "score blend nn val set\n",
      "0.99533146592\n",
      "rNNTe\n",
      "[ 0.93843284  0.          0.94496269]\n",
      "rNNVa\n",
      "[ 0.97759104  0.          0.99533147]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  \n",
    "          \n",
    "    print(\"blend nn\")\n",
    "    bmdl.fitNN(vaX,vaY,lambda1=0.000000,lambda2=0.0000,teX=teX,teY=teY)\n",
    "\n",
    "    predictTe=bmdl.predict_NNproba(teX)\n",
    "    scoreTe=np.mean(np.argmax(predictTe, axis=1) == teY) \n",
    "    print(\"score blend nn\")\n",
    "    print(scoreTe)\n",
    "    rNNTe[j]=scoreTe\n",
    "\n",
    "    predictVa=bmdl.predict_NNproba(vaX)\n",
    "    scoreVa=np.mean(np.argmax(predictVa, axis=1) == vaY) \n",
    "    print(\"score blend nn val set\")\n",
    "    print(scoreVa)\n",
    "    rNNVa[j]=scoreVa\n",
    "\n",
    "    print \"rNNTe\"\n",
    "    print rNNTe\n",
    "    print \"rNNVa\"\n",
    "    print rNNVa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
