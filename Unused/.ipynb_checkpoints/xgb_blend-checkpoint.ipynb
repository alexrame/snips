{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_ntrees</th>\n",
       "      <th>best_score</th>\n",
       "      <th>bst:colsample_bytree</th>\n",
       "      <th>bst:eta</th>\n",
       "      <th>bst:max_depth</th>\n",
       "      <th>bst:subsample</th>\n",
       "      <th>early_stopping</th>\n",
       "      <th>n_round</th>\n",
       "      <th>ntrees</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>134</td>\n",
       "      <td>0.059785</td>\n",
       "      <td>0.943942</td>\n",
       "      <td>0.115512</td>\n",
       "      <td>18</td>\n",
       "      <td>0.647034</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>234</td>\n",
       "      <td>0.061653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>254</td>\n",
       "      <td>0.066324</td>\n",
       "      <td>0.858151</td>\n",
       "      <td>0.203779</td>\n",
       "      <td>17</td>\n",
       "      <td>0.585147</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>354</td>\n",
       "      <td>0.066791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>147</td>\n",
       "      <td>0.066324</td>\n",
       "      <td>0.799230</td>\n",
       "      <td>0.153808</td>\n",
       "      <td>13</td>\n",
       "      <td>0.536295</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>247</td>\n",
       "      <td>0.071929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>152</td>\n",
       "      <td>0.066324</td>\n",
       "      <td>0.780146</td>\n",
       "      <td>0.097607</td>\n",
       "      <td>16</td>\n",
       "      <td>0.755971</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>252</td>\n",
       "      <td>0.069594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>88</td>\n",
       "      <td>0.066791</td>\n",
       "      <td>0.760885</td>\n",
       "      <td>0.204191</td>\n",
       "      <td>18</td>\n",
       "      <td>0.575165</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>188</td>\n",
       "      <td>0.070528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>103</td>\n",
       "      <td>0.067258</td>\n",
       "      <td>0.820301</td>\n",
       "      <td>0.138812</td>\n",
       "      <td>28</td>\n",
       "      <td>0.656281</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>203</td>\n",
       "      <td>0.071462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>73</td>\n",
       "      <td>0.067725</td>\n",
       "      <td>0.743723</td>\n",
       "      <td>0.126824</td>\n",
       "      <td>25</td>\n",
       "      <td>0.510473</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>173</td>\n",
       "      <td>0.072863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>50</td>\n",
       "      <td>0.067725</td>\n",
       "      <td>0.732633</td>\n",
       "      <td>0.172330</td>\n",
       "      <td>13</td>\n",
       "      <td>0.760958</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>150</td>\n",
       "      <td>0.069594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>199</td>\n",
       "      <td>0.068192</td>\n",
       "      <td>0.773489</td>\n",
       "      <td>0.140193</td>\n",
       "      <td>12</td>\n",
       "      <td>0.575883</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>299</td>\n",
       "      <td>0.071929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>59</td>\n",
       "      <td>0.068192</td>\n",
       "      <td>0.968865</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>16</td>\n",
       "      <td>0.510009</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>159</td>\n",
       "      <td>0.071462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>92</td>\n",
       "      <td>0.068192</td>\n",
       "      <td>0.770574</td>\n",
       "      <td>0.141824</td>\n",
       "      <td>25</td>\n",
       "      <td>0.717632</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>192</td>\n",
       "      <td>0.072396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>147</td>\n",
       "      <td>0.068192</td>\n",
       "      <td>0.720356</td>\n",
       "      <td>0.149401</td>\n",
       "      <td>15</td>\n",
       "      <td>0.528869</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>247</td>\n",
       "      <td>0.071462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>382</td>\n",
       "      <td>0.068660</td>\n",
       "      <td>0.730941</td>\n",
       "      <td>0.022750</td>\n",
       "      <td>11</td>\n",
       "      <td>0.507739</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>482</td>\n",
       "      <td>0.070061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>151</td>\n",
       "      <td>0.068660</td>\n",
       "      <td>0.766971</td>\n",
       "      <td>0.075620</td>\n",
       "      <td>29</td>\n",
       "      <td>0.653425</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>251</td>\n",
       "      <td>0.070995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>49</td>\n",
       "      <td>0.068660</td>\n",
       "      <td>0.903681</td>\n",
       "      <td>0.194792</td>\n",
       "      <td>12</td>\n",
       "      <td>0.866647</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>149</td>\n",
       "      <td>0.072396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>118</td>\n",
       "      <td>0.068660</td>\n",
       "      <td>0.762266</td>\n",
       "      <td>0.124683</td>\n",
       "      <td>22</td>\n",
       "      <td>0.528421</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>218</td>\n",
       "      <td>0.074264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>332</td>\n",
       "      <td>0.069127</td>\n",
       "      <td>0.912202</td>\n",
       "      <td>0.083739</td>\n",
       "      <td>17</td>\n",
       "      <td>0.795222</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>432</td>\n",
       "      <td>0.070061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>54</td>\n",
       "      <td>0.069127</td>\n",
       "      <td>0.882756</td>\n",
       "      <td>0.178620</td>\n",
       "      <td>13</td>\n",
       "      <td>0.587791</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>154</td>\n",
       "      <td>0.071929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>232</td>\n",
       "      <td>0.069127</td>\n",
       "      <td>0.819083</td>\n",
       "      <td>0.069690</td>\n",
       "      <td>13</td>\n",
       "      <td>0.532094</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>332</td>\n",
       "      <td>0.070061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>110</td>\n",
       "      <td>0.069594</td>\n",
       "      <td>0.844764</td>\n",
       "      <td>0.127157</td>\n",
       "      <td>12</td>\n",
       "      <td>0.623768</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>210</td>\n",
       "      <td>0.070995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>185</td>\n",
       "      <td>0.069594</td>\n",
       "      <td>0.789434</td>\n",
       "      <td>0.055601</td>\n",
       "      <td>17</td>\n",
       "      <td>0.588469</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>285</td>\n",
       "      <td>0.071929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>137</td>\n",
       "      <td>0.069594</td>\n",
       "      <td>0.991373</td>\n",
       "      <td>0.122698</td>\n",
       "      <td>18</td>\n",
       "      <td>0.896203</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>237</td>\n",
       "      <td>0.072863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>55</td>\n",
       "      <td>0.069594</td>\n",
       "      <td>0.700731</td>\n",
       "      <td>0.174158</td>\n",
       "      <td>28</td>\n",
       "      <td>0.766643</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>155</td>\n",
       "      <td>0.073330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>466</td>\n",
       "      <td>0.069594</td>\n",
       "      <td>0.932822</td>\n",
       "      <td>0.040639</td>\n",
       "      <td>14</td>\n",
       "      <td>0.557058</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>566</td>\n",
       "      <td>0.070061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>246</td>\n",
       "      <td>0.069594</td>\n",
       "      <td>0.774223</td>\n",
       "      <td>0.189640</td>\n",
       "      <td>15</td>\n",
       "      <td>0.647990</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>346</td>\n",
       "      <td>0.070528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>82</td>\n",
       "      <td>0.069594</td>\n",
       "      <td>0.983171</td>\n",
       "      <td>0.139885</td>\n",
       "      <td>19</td>\n",
       "      <td>0.565108</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>182</td>\n",
       "      <td>0.071929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>119</td>\n",
       "      <td>0.069594</td>\n",
       "      <td>0.748431</td>\n",
       "      <td>0.073596</td>\n",
       "      <td>10</td>\n",
       "      <td>0.657175</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>219</td>\n",
       "      <td>0.072863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     best_ntrees  best_score  bst:colsample_bytree   bst:eta  bst:max_depth  \\\n",
       "6            134    0.059785              0.943942  0.115512             18   \n",
       "0            254    0.066324              0.858151  0.203779             17   \n",
       "46           147    0.066324              0.799230  0.153808             13   \n",
       "113          152    0.066324              0.780146  0.097607             16   \n",
       "89            88    0.066791              0.760885  0.204191             18   \n",
       "132          103    0.067258              0.820301  0.138812             28   \n",
       "62            73    0.067725              0.743723  0.126824             25   \n",
       "87            50    0.067725              0.732633  0.172330             13   \n",
       "18           199    0.068192              0.773489  0.140193             12   \n",
       "131           59    0.068192              0.968865  0.164062             16   \n",
       "90            92    0.068192              0.770574  0.141824             25   \n",
       "7            147    0.068192              0.720356  0.149401             15   \n",
       "119          382    0.068660              0.730941  0.022750             11   \n",
       "53           151    0.068660              0.766971  0.075620             29   \n",
       "72            49    0.068660              0.903681  0.194792             12   \n",
       "120          118    0.068660              0.762266  0.124683             22   \n",
       "24           332    0.069127              0.912202  0.083739             17   \n",
       "37            54    0.069127              0.882756  0.178620             13   \n",
       "133          232    0.069127              0.819083  0.069690             13   \n",
       "121          110    0.069594              0.844764  0.127157             12   \n",
       "73           185    0.069594              0.789434  0.055601             17   \n",
       "98           137    0.069594              0.991373  0.122698             18   \n",
       "51            55    0.069594              0.700731  0.174158             28   \n",
       "34           466    0.069594              0.932822  0.040639             14   \n",
       "1            246    0.069594              0.774223  0.189640             15   \n",
       "100           82    0.069594              0.983171  0.139885             19   \n",
       "134          119    0.069594              0.748431  0.073596             10   \n",
       "\n",
       "     bst:subsample  early_stopping  n_round  ntrees     score  \n",
       "6         0.647034             100      800     234  0.061653  \n",
       "0         0.585147             100      800     354  0.066791  \n",
       "46        0.536295             100      800     247  0.071929  \n",
       "113       0.755971             100      800     252  0.069594  \n",
       "89        0.575165             100      800     188  0.070528  \n",
       "132       0.656281             100      800     203  0.071462  \n",
       "62        0.510473             100      800     173  0.072863  \n",
       "87        0.760958             100      800     150  0.069594  \n",
       "18        0.575883             100      800     299  0.071929  \n",
       "131       0.510009             100      800     159  0.071462  \n",
       "90        0.717632             100      800     192  0.072396  \n",
       "7         0.528869             100      800     247  0.071462  \n",
       "119       0.507739             100      800     482  0.070061  \n",
       "53        0.653425             100      800     251  0.070995  \n",
       "72        0.866647             100      800     149  0.072396  \n",
       "120       0.528421             100      800     218  0.074264  \n",
       "24        0.795222             100      800     432  0.070061  \n",
       "37        0.587791             100      800     154  0.071929  \n",
       "133       0.532094             100      800     332  0.070061  \n",
       "121       0.623768             100      800     210  0.070995  \n",
       "73        0.588469             100      800     285  0.071929  \n",
       "98        0.896203             100      800     237  0.072863  \n",
       "51        0.766643             100      800     155  0.073330  \n",
       "34        0.557058             100      800     566  0.070061  \n",
       "1         0.647990             100      800     346  0.070528  \n",
       "100       0.565108             100      800     182  0.071929  \n",
       "134       0.657175             100      800     219  0.072863  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os.path\n",
    "import scipy\n",
    "import random\n",
    "from loadfft import getData\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble.base import BaseEnsemble\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from blend_nn import NeuralNet\n",
    "\n",
    "results = pd.io.pickle.read_pickle('results/results_df_last_fft.pkl')\n",
    "models_to_use = results[(results['best_score']<0.07) | ((results['best_score']<0.09)&(results['bst:eta']>0.3))]\n",
    "models_to_use=models_to_use.sort(['best_score'])\n",
    "models_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2224\n",
      "3106\n",
      "536\n",
      "759\n"
     ]
    }
   ],
   "source": [
    "trX,trY,teX,teY = getData(oh=0)\n",
    "l=len(trX)\n",
    "cut=l*0\n",
    "\n",
    "vaX=trX[0:cut,]\n",
    "vaY=trY[:cut]\n",
    "trX=trX[cut:,:]\n",
    "trY=trY[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModifiedXGBClassifier(xgb.XGBClassifier):\n",
    "    def __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, \n",
    "                 silent=True, objective=\"reg:linear\", max_features=1, subsample = 1):\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.silent = silent\n",
    "        self.n_estimators = n_estimators\n",
    "        self.objective = objective\n",
    "        self.max_features = max_features\n",
    "        self.subsample = subsample\n",
    "        self._Booster = xgb.Booster()\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return {'max_depth': self.max_depth,\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'n_estimators': self.n_estimators,\n",
    "                'silent': self.silent,\n",
    "                'objective': self.objective,\n",
    "                'max_features' : self.max_features,\n",
    "                'subsample' : self.subsample\n",
    "                }\n",
    "        \n",
    "    def get_xgb_params(self):\n",
    "        return {'eta': self.learning_rate,\n",
    "                'max_depth': self.max_depth,\n",
    "                'silent': 1 if self.silent else 0,\n",
    "                'objective': self.objective,\n",
    "                'bst:subsample': self.subsample,\n",
    "                'bst:colsample_bytree': self.max_features\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BlendedModel(BaseEnsemble):\n",
    "    def __init__(self, models=[], blending='average'):\n",
    "        self.models = models\n",
    "        self.blending = blending\n",
    "        self.logR = LogisticRegression()\n",
    "        self.logRT= LogisticRegression()\n",
    "        self.nn=NeuralNet(len(models)*4)\n",
    "        if self.blending not in ['average', 'most_confident']:\n",
    "            raise Exception('Wrong blending method')\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for model in self.models:\n",
    "            print 'Training model :'\n",
    "            print model.get_params()\n",
    "            model.fit(X, y)                \n",
    "        return self\n",
    "    \n",
    "    def fitLog(self,X,y,mod=0):\n",
    "        if mod==0:\n",
    "            preds = np.array(\n",
    "                        [model.predict_proba(X) for model in self.models]\n",
    "                    )\n",
    "            features=np.array([np.array([preds[j][i] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "            #features = preprocessing.scale(features)\n",
    "            self.logR.fit(features, y)\n",
    "        elif mod==1:\n",
    "            preds = np.array(\n",
    "                        [model.predict_proba(X) for model in self.models]\n",
    "                    )\n",
    "            features=np.array([np.array([[math.log(preds[j][i][k]/(1-preds[j][i][k])) for k in range(4)] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "            #features = preprocessing.scale(features)\n",
    "            self.logRT.fit(features, y)\n",
    "        return self\n",
    "    def fitNN(self,X,y):\n",
    "        preds = np.array(\n",
    "                        [model.predict_proba(X) for model in self.models]\n",
    "                    )\n",
    "        features=np.array([np.array([preds[j][i] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "        #features = preprocessing.scale(features)\n",
    "        self.nn.fit(features,y)\n",
    "        return self\n",
    "    def predict_NNproba(self,X):\n",
    "        preds = np.array(\n",
    "                        [model.predict_proba(X) for model in self.models]\n",
    "                    )\n",
    "        features=np.array([np.array([preds[j][i] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "        #features = preprocessing.scale(features)\n",
    "        return self.nn.predict_proba(features)\n",
    "    def predict_proba(self, X):\n",
    "        preds = np.array(\n",
    "                    [model.predict_proba(X) for model in self.models]\n",
    "                )\n",
    "        if self.blending == 'average':\n",
    "            return np.mean(preds , axis=0 )\n",
    "        elif self.blending == 'most_confident':\n",
    "            def dirac_weights(entropies):\n",
    "                w = (entropies == np.min(entropies)).astype(float)\n",
    "                return w/np.sum(w)\n",
    "            def shannon_entropy(l):\n",
    "                l = [min(max(1e-5,p),1-1e-5) for p in l]\n",
    "                l = np.array(l)/sum(l)\n",
    "                return sum([-p*math.log(p) for p in l])\n",
    "            shannon_entropy_array = lambda l : np.apply_along_axis(shannon_entropy, 1, l)\n",
    "            entropies = np.array([shannon_entropy_array(pred) for pred in preds])\n",
    "            weights = np.apply_along_axis(dirac_weights, 0, entropies)\n",
    "            return np.sum(np.multiply(weights.T, preds.T).T, axis = 0)\n",
    "    \n",
    "    def predict_Logproba(self, X,mod=0):\n",
    "        if mod==0:\n",
    "            preds = np.array(\n",
    "                        [model.predict_proba(X) for model in self.models]\n",
    "                    )\n",
    "            features=np.array([np.array([preds[j][i] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "            #features = preprocessing.scale(features)\n",
    "            preds=self.logR.predict_proba(features)\n",
    "            return preds\n",
    "        elif mod==1:\n",
    "            preds = np.array(\n",
    "                    [model.predict_proba(X) for model in self.models]\n",
    "                )\n",
    "            features=np.array([np.array([[math.log(preds[j][i][k]/(1-preds[j][i][k])) for k in range(4)] for j in range(len(self.models))]).flatten() for i in range(len(X))])\n",
    "            #features = preprocessing.scale(features)\n",
    "            preds=self.logRT.predict_proba(features)\n",
    "            return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ModifiedXGBClassifier(learning_rate=0.11551197421096039, max_depth=18.0,\n",
       "            max_features=0.94394155601070184, n_estimators=134,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.64703405479748977),\n",
       " ModifiedXGBClassifier(learning_rate=0.20377858764434154, max_depth=17.0,\n",
       "            max_features=0.85815076222488917, n_estimators=254,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.58514739013739292),\n",
       " ModifiedXGBClassifier(learning_rate=0.15380842512268544, max_depth=13.0,\n",
       "            max_features=0.79923049310131822, n_estimators=147,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.53629487294143696),\n",
       " ModifiedXGBClassifier(learning_rate=0.09760719357142815, max_depth=16.0,\n",
       "            max_features=0.78014637555483268, n_estimators=152,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.75597110990741156),\n",
       " ModifiedXGBClassifier(learning_rate=0.20419134525467328, max_depth=18.0,\n",
       "            max_features=0.76088467322508702, n_estimators=88,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.57516472819641407),\n",
       " ModifiedXGBClassifier(learning_rate=0.13881200895631374, max_depth=28.0,\n",
       "            max_features=0.82030144812493166, n_estimators=103,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.65628110357564529),\n",
       " ModifiedXGBClassifier(learning_rate=0.12682398580791093, max_depth=25.0,\n",
       "            max_features=0.74372326675265921, n_estimators=73,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.51047280801423534),\n",
       " ModifiedXGBClassifier(learning_rate=0.17233041645303759, max_depth=13.0,\n",
       "            max_features=0.73263258346622184, n_estimators=50,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.76095807370501523),\n",
       " ModifiedXGBClassifier(learning_rate=0.1401927742474989, max_depth=12.0,\n",
       "            max_features=0.77348879395028192, n_estimators=199,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.57588341184138359),\n",
       " ModifiedXGBClassifier(learning_rate=0.16406241316515649, max_depth=16.0,\n",
       "            max_features=0.96886531891846239, n_estimators=59,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.51000879745277872),\n",
       " ModifiedXGBClassifier(learning_rate=0.14182442242417781, max_depth=25.0,\n",
       "            max_features=0.77057350490863552, n_estimators=92,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.71763160505735024),\n",
       " ModifiedXGBClassifier(learning_rate=0.14940109157987036, max_depth=15.0,\n",
       "            max_features=0.72035550407487636, n_estimators=147,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.528869396697074),\n",
       " ModifiedXGBClassifier(learning_rate=0.022750107823934398, max_depth=11.0,\n",
       "            max_features=0.73094078133628593, n_estimators=382,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.50773876545771079),\n",
       " ModifiedXGBClassifier(learning_rate=0.075619829839716518, max_depth=29.0,\n",
       "            max_features=0.76697140588581081, n_estimators=151,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.65342544729801588),\n",
       " ModifiedXGBClassifier(learning_rate=0.19479235667330097, max_depth=12.0,\n",
       "            max_features=0.90368111705755427, n_estimators=49,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.86664714687244038),\n",
       " ModifiedXGBClassifier(learning_rate=0.12468274157649865, max_depth=22.0,\n",
       "            max_features=0.76226562083027416, n_estimators=118,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.52842084976231285),\n",
       " ModifiedXGBClassifier(learning_rate=0.083739321401597691, max_depth=17.0,\n",
       "            max_features=0.91220204160578189, n_estimators=332,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.79522183039896011),\n",
       " ModifiedXGBClassifier(learning_rate=0.17861980248016807, max_depth=13.0,\n",
       "            max_features=0.88275631799894883, n_estimators=54,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.5877909974681117),\n",
       " ModifiedXGBClassifier(learning_rate=0.069689512805042153, max_depth=13.0,\n",
       "            max_features=0.81908347648442004, n_estimators=232,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.53209449397046615),\n",
       " ModifiedXGBClassifier(learning_rate=0.12715716317257603, max_depth=12.0,\n",
       "            max_features=0.84476366165995587, n_estimators=110,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.62376811277812716),\n",
       " ModifiedXGBClassifier(learning_rate=0.055601240897976065, max_depth=17.0,\n",
       "            max_features=0.78943433337231861, n_estimators=185,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.58846941192166224),\n",
       " ModifiedXGBClassifier(learning_rate=0.12269783587663279, max_depth=18.0,\n",
       "            max_features=0.99137262775665835, n_estimators=137,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.89620296651683917),\n",
       " ModifiedXGBClassifier(learning_rate=0.17415793628270348, max_depth=28.0,\n",
       "            max_features=0.70073148056692791, n_estimators=55,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.76664298501798966),\n",
       " ModifiedXGBClassifier(learning_rate=0.040638727240712373, max_depth=14.0,\n",
       "            max_features=0.93282245051452439, n_estimators=466,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.55705811280166906),\n",
       " ModifiedXGBClassifier(learning_rate=0.1896403761530398, max_depth=15.0,\n",
       "            max_features=0.77422329725142658, n_estimators=246,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.64798980547803731),\n",
       " ModifiedXGBClassifier(learning_rate=0.13988514131042132, max_depth=19.0,\n",
       "            max_features=0.98317120307981765, n_estimators=82,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.56510838440342392),\n",
       " ModifiedXGBClassifier(learning_rate=0.073596026969053402, max_depth=10.0,\n",
       "            max_features=0.74843113504320591, n_estimators=119,\n",
       "            objective='multi:softprob', silent=True,\n",
       "            subsample=0.6571749411893919)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = []\n",
    "for j,row in enumerate(models_to_use.iterrows()):\n",
    "    print j\n",
    "    hyperparams = dict(row[1])\n",
    "    models.append(ModifiedXGBClassifier(\n",
    "                            max_depth=hyperparams['bst:max_depth'], \n",
    "                            learning_rate=hyperparams['bst:eta'], \n",
    "                            n_estimators=int(hyperparams['best_ntrees']),\n",
    "                            max_features=hyperparams['bst:colsample_bytree'],\n",
    "                            subsample=hyperparams['bst:subsample'],\n",
    "                            silent=True, \n",
    "                            objective='multi:softprob')\n",
    "                  )\n",
    "models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model :\n",
      "{'silent': True, 'subsample': 0.64703405479748977, 'learning_rate': 0.11551197421096039, 'n_estimators': 20, 'max_features': 0.94394155601070184, 'objective': 'multi:softprob', 'max_depth': 18.0}\n",
      "Training model :\n",
      "{'silent': True, 'subsample': 0.58514739013739292, 'learning_rate': 0.20377858764434154, 'n_estimators': 20, 'max_features': 0.85815076222488917, 'objective': 'multi:softprob', 'max_depth': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/notebook/.venv/lib/python2.7/site-packages/xgboost-0.32-py2.7.egg/xgboost.py:157: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  data = np.array(mat.reshape(mat.size), dtype=np.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BlendedModel(blending='average',\n",
       "       models=[ModifiedXGBClassifier(learning_rate=0.11551197421096039, max_depth=18.0,\n",
       "           max_features=0.94394155601070184, n_estimators=20,\n",
       "           objective='multi:softprob', silent=True,\n",
       "           subsample=0.64703405479748977), ModifiedXGBClassifier(learning_rate=0.20377858764434154, max_depth=17.0,\n",
       "           max_features=0.85815076222488917, n_estimators=20,\n",
       "           objective='multi:softprob', silent=True,\n",
       "           subsample=0.58514739013739292)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmdl = BlendedModel(models)\n",
    "bmdl.fit(trX,trY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score des differents modeles\n",
      "0.85987856142\n",
      "0.862680990191\n"
     ]
    }
   ],
   "source": [
    "print(\"score des differents modeles\")\n",
    "for model in models:\n",
    "    print(np.mean(np.argmax(model.predict_proba(teX), axis=1) == teY))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score sur la moyenne des differents modeles\n",
      "0.865016347501\n"
     ]
    }
   ],
   "source": [
    "predictTe=bmdl.predict_proba(teX)\n",
    "scoreTe=np.mean(np.argmax(predictTe, axis=1) == teY) \n",
    "print(\"score sur la moyenne des differents modeles\")\n",
    "print(scoreTe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blend logistic regression\n",
      "score blend logistic regression\n",
      "0.870154133582\n",
      "score blend logistic regression val test\n",
      "0.865732632808\n"
     ]
    }
   ],
   "source": [
    "print(\"blend logistic regression\")\n",
    "bmdl.fitLog(vaX,vaY)\n",
    "predictTeL=bmdl.predict_Logproba(teX)\n",
    "scoreTeL=np.mean(np.argmax(predictTeL, axis=1) == teY) \n",
    "print(\"score blend logistic regression\")\n",
    "print(scoreTeL)\n",
    "predictVaL=bmdl.predict_Logproba(vaX)\n",
    "scoreVaL=np.mean(np.argmax(predictVaL, axis=1) == vaY) \n",
    "print(\"score blend logistic regression val test\")\n",
    "print(scoreVaL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blend logistic regression with log(x/(1-x)\n",
      "score blend logistic regression with log(x/(1-x)\n",
      "0.875758991126\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(\"blend logistic regression with log(x/(1-x)\")\n",
    "bmdl.fitLog(vaX,vaY,mod=1)\n",
    "predictTeL=bmdl.predict_Logproba(teX,mod=1)\n",
    "scoreTeL=np.mean(np.argmax(predictTeL, axis=1) == teY) \n",
    "print(\"score blend logistic regression with log(x/(1-x)\")\n",
    "print(scoreTeL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blend logistic regression with log(x/(1-x)\n",
      "score blend logistic regression with log(x/(1-x) val test\n",
      "0.86660828955\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(\"blend logistic regression with log(x/(1-x)\")\n",
    "predictVaL=bmdl.predict_Logproba(vaX,mod=1)\n",
    "scoreVaL=np.mean(np.argmax(predictVaL, axis=1) == vaY) \n",
    "print(\"score blend logistic regression with log(x/(1-x) val test\")\n",
    "print(scoreVaL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blend nn\n",
      "1000 3 100 0.0 0.0\n",
      "3426\n",
      "50 3 0.92119089317 0.332678195701\n",
      "100 3 0.923234092236 0.331736259725\n",
      "150 3 0.907764156451 0.383622712641\n",
      "200 3 0.913018096906 0.453299786582\n",
      "250 3 0.906304728546 0.497605131131\n",
      "300 3 0.926152948044 0.35361344257\n",
      "350 3 0.906012842966 0.486000995051\n",
      "400 3 0.927904261529 0.294094355944\n",
      "450 3 0.923817863398 0.35618066646\n",
      "500 3 0.934033858727 0.285077570489\n",
      "550 3 0.933158201985 0.355116227444\n",
      "600 3 0.925277291302 0.388622822674\n",
      "650 3 0.92819614711 0.514564685446\n",
      "700 3 0.922358435493 0.418172264738\n",
      "750 3 0.936952714536 0.310220887796\n",
      "800 3 0.921774664332 0.485793205803\n",
      "850 3 0.9176882662 0.395951754026\n",
      "900 3 0.934617629889 0.314578548014\n",
      "950 3 0.92469352014 0.396100629105\n",
      "1000 3 0.917980151781 0.424617771886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BlendedModel(blending='average',\n",
       "       models=[ModifiedXGBClassifier(learning_rate=0.11551197421096039, max_depth=18.0,\n",
       "           max_features=0.94394155601070184, n_estimators=20,\n",
       "           objective='multi:softprob', silent=True,\n",
       "           subsample=0.64703405479748977), ModifiedXGBClassifier(learning_rate=0.20377858764434154, max_depth=17.0,\n",
       "           max_features=0.85815076222488917, n_estimators=20,\n",
       "           objective='multi:softprob', silent=True,\n",
       "           subsample=0.58514739013739292)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"blend nn\")\n",
    "bmdl.fitNN(vaX,vaY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score blend nn\n",
      "0.846333489024\n"
     ]
    }
   ],
   "source": [
    "predictTeL=bmdl.predict_NNproba(teX)\n",
    "predictTeL\n",
    "scoreTeL=np.mean(np.argmax(predictTeL, axis=1) == teY) \n",
    "print(\"score blend nn\")\n",
    "print(scoreTeL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score blend nn val set\n",
      "0.917980151781\n"
     ]
    }
   ],
   "source": [
    "predictVaL=bmdl.predict_NNproba(vaX)\n",
    "\n",
    "scoreVaL=np.mean(np.argmax(predictVaL, axis=1) == vaY) \n",
    "print(\"score blend nn val set\")\n",
    "print(scoreVaL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_ntrees</th>\n",
       "      <th>best_score</th>\n",
       "      <th>bst:colsample_bytree</th>\n",
       "      <th>bst:eta</th>\n",
       "      <th>bst:max_depth</th>\n",
       "      <th>bst:subsample</th>\n",
       "      <th>early_stopping</th>\n",
       "      <th>n_round</th>\n",
       "      <th>ntrees</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>134</td>\n",
       "      <td>0.059785</td>\n",
       "      <td>0.943942</td>\n",
       "      <td>0.115512</td>\n",
       "      <td>18</td>\n",
       "      <td>0.647034</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>234</td>\n",
       "      <td>0.061653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>254</td>\n",
       "      <td>0.066324</td>\n",
       "      <td>0.858151</td>\n",
       "      <td>0.203779</td>\n",
       "      <td>17</td>\n",
       "      <td>0.585147</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>354</td>\n",
       "      <td>0.066791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   best_ntrees  best_score  bst:colsample_bytree   bst:eta  bst:max_depth  \\\n",
       "6          134    0.059785              0.943942  0.115512             18   \n",
       "0          254    0.066324              0.858151  0.203779             17   \n",
       "\n",
       "   bst:subsample  early_stopping  n_round  ntrees     score  \n",
       "6       0.647034             100      800     234  0.061653  \n",
       "0       0.585147             100      800     354  0.066791  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1=models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModifiedXGBClassifier(learning_rate=0.11551197421096039, max_depth=18.0,\n",
       "           max_features=0.94394155601070184, n_estimators=134,\n",
       "           objective='multi:softprob', silent=True,\n",
       "           subsample=0.64703405479748977)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModifiedXGBClassifier(learning_rate=0.11551197421096039, max_depth=18.0,\n",
       "           max_features=0.94394155601070184, n_estimators=134,\n",
       "           objective='multi:softprob', silent=True,\n",
       "           subsample=0.64703405479748977)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(trX,trY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.922466137319\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.argmax(model1.predict_proba(teX), axis=1) == teY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_ntrees': 134.0,\n",
       " 'best_score': 0.059784999999999998,\n",
       " 'bst:colsample_bytree': 0.94394155601070184,\n",
       " 'bst:eta': 0.11551197421096039,\n",
       " 'bst:max_depth': 18.0,\n",
       " 'bst:subsample': 0.64703405479748977,\n",
       " 'early_stopping': 100.0,\n",
       " 'n_round': 800.0,\n",
       " 'ntrees': 234.0,\n",
       " 'score': 0.061652999999999999}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hyperparams=model1.get_xgb_params()\n",
    "hyperparams = dict(models_to_use.iloc[0])\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bst:colsample_bytree': 0.94394155601070184,\n",
       " 'bst:subsample': 0.64703405479748977,\n",
       " 'eta': 0.11551197421096039,\n",
       " 'max_depth': 18.0,\n",
       " 'objective': 'multi:softprob',\n",
       " 'silent': 1}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams=model1.get_xgb_params()\n",
    "n_round = int(model1.get_params()['n_estimators'])\n",
    "n_round\n",
    "hyperparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.11551197421096039,\n",
       " 'max_depth': 18.0,\n",
       " 'max_features': 0.94394155601070184,\n",
       " 'n_estimators': 134,\n",
       " 'objective': 'multi:softprob',\n",
       " 'silent': True,\n",
       " 'subsample': 0.64703405479748977}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hyperparams['n_round'] = int(hyperparams['best_ntrees'])\n",
    "#hyperparams['bst:max_depth'] = int(hyperparams['bst:max_depth'])\n",
    "\n",
    "dtrain = xgb.DMatrix(trX, label = trY)\n",
    "dtest = xgb.DMatrix(teX, label = teY)\n",
    "\n",
    "n_round = int(model1.get_params()['n_estimators'])\n",
    "plst = [\n",
    "        ('bst:max_depth', hyperparams['max_depth']),\n",
    "        ('objective', 'multi:softprob'),\n",
    "        ('silent', 1),\n",
    "        ('bst:eta', model1.get_params()['learning_rate']),\n",
    "        ('bst:subsample', hyperparams['bst:subsample']),\n",
    "        ('bst:colsample_bytree', hyperparams['bst:colsample_bytree']),\n",
    "        ('num_class', 4)\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]\ttest-merror:0.160206\n",
      "[1]\ttest-merror:0.125642\n",
      "[2]\ttest-merror:0.114900\n",
      "[3]\ttest-merror:0.117702\n",
      "[4]\ttest-merror:0.113498\n",
      "[5]\ttest-merror:0.109762\n",
      "[6]\ttest-merror:0.106492\n",
      "[7]\ttest-merror:0.106492\n",
      "[8]\ttest-merror:0.103690\n",
      "[9]\ttest-merror:0.105558\n",
      "[10]\ttest-merror:0.104157\n",
      "[11]\ttest-merror:0.103690\n",
      "[12]\ttest-merror:0.101355\n",
      "[13]\ttest-merror:0.100420\n",
      "[14]\ttest-merror:0.098552\n",
      "[15]\ttest-merror:0.099019\n",
      "[16]\ttest-merror:0.097151\n",
      "[17]\ttest-merror:0.097151\n",
      "[18]\ttest-merror:0.098085\n",
      "[19]\ttest-merror:0.096217\n",
      "[20]\ttest-merror:0.096684\n",
      "[21]\ttest-merror:0.094348\n",
      "[22]\ttest-merror:0.094348\n",
      "[23]\ttest-merror:0.092480\n",
      "[24]\ttest-merror:0.093881\n",
      "[25]\ttest-merror:0.090612\n",
      "[26]\ttest-merror:0.090145\n",
      "[27]\ttest-merror:0.089211\n",
      "[28]\ttest-merror:0.086875\n",
      "[29]\ttest-merror:0.087342\n",
      "[30]\ttest-merror:0.086408\n",
      "[31]\ttest-merror:0.086408\n",
      "[32]\ttest-merror:0.085474\n",
      "[33]\ttest-merror:0.085474\n",
      "[34]\ttest-merror:0.085474\n",
      "[35]\ttest-merror:0.085474\n",
      "[36]\ttest-merror:0.085474\n",
      "[37]\ttest-merror:0.085474\n",
      "[38]\ttest-merror:0.085941\n",
      "[39]\ttest-merror:0.085941\n",
      "[40]\ttest-merror:0.084540\n",
      "[41]\ttest-merror:0.083606\n",
      "[42]\ttest-merror:0.083139\n",
      "[43]\ttest-merror:0.083606\n",
      "[44]\ttest-merror:0.082672\n",
      "[45]\ttest-merror:0.083606\n",
      "[46]\ttest-merror:0.082205\n",
      "[47]\ttest-merror:0.080336\n",
      "[48]\ttest-merror:0.080336\n",
      "[49]\ttest-merror:0.080336\n",
      "[50]\ttest-merror:0.079869\n",
      "[51]\ttest-merror:0.080803\n",
      "[52]\ttest-merror:0.080803\n",
      "[53]\ttest-merror:0.080336\n",
      "[54]\ttest-merror:0.079402\n",
      "[55]\ttest-merror:0.080803\n",
      "[56]\ttest-merror:0.079869\n",
      "[57]\ttest-merror:0.078935\n",
      "[58]\ttest-merror:0.078935\n",
      "[59]\ttest-merror:0.078935\n",
      "[60]\ttest-merror:0.079869\n",
      "[61]\ttest-merror:0.078935\n",
      "[62]\ttest-merror:0.078935\n",
      "[63]\ttest-merror:0.078468\n",
      "[64]\ttest-merror:0.079402\n",
      "[65]\ttest-merror:0.077067\n",
      "[66]\ttest-merror:0.078001\n",
      "[67]\ttest-merror:0.077534\n",
      "[68]\ttest-merror:0.077067\n",
      "[69]\ttest-merror:0.076600\n",
      "[70]\ttest-merror:0.075666\n",
      "[71]\ttest-merror:0.076600\n",
      "[72]\ttest-merror:0.077534\n",
      "[73]\ttest-merror:0.077067\n",
      "[74]\ttest-merror:0.077067\n",
      "[75]\ttest-merror:0.077067\n",
      "[76]\ttest-merror:0.077534\n",
      "[77]\ttest-merror:0.077067\n",
      "[78]\ttest-merror:0.077534\n",
      "[79]\ttest-merror:0.077534\n",
      "[80]\ttest-merror:0.077534\n",
      "[81]\ttest-merror:0.077534\n",
      "[82]\ttest-merror:0.077534\n",
      "[83]\ttest-merror:0.077534\n",
      "[84]\ttest-merror:0.077534\n",
      "[85]\ttest-merror:0.078001\n",
      "[86]\ttest-merror:0.078001\n",
      "[87]\ttest-merror:0.078935\n",
      "[88]\ttest-merror:0.078001\n",
      "[89]\ttest-merror:0.078468\n",
      "[90]\ttest-merror:0.078001\n",
      "[91]\ttest-merror:0.078001\n",
      "[92]\ttest-merror:0.078001\n",
      "[93]\ttest-merror:0.077534\n",
      "[94]\ttest-merror:0.077534\n",
      "[95]\ttest-merror:0.078001\n",
      "[96]\ttest-merror:0.078001\n",
      "[97]\ttest-merror:0.078001\n",
      "[98]\ttest-merror:0.078001\n",
      "[99]\ttest-merror:0.078001\n",
      "[100]\ttest-merror:0.077534\n",
      "[101]\ttest-merror:0.077534\n",
      "[102]\ttest-merror:0.077534\n",
      "[103]\ttest-merror:0.077067\n",
      "[104]\ttest-merror:0.077534\n",
      "[105]\ttest-merror:0.077534\n",
      "[106]\ttest-merror:0.077534\n",
      "[107]\ttest-merror:0.077534\n",
      "[108]\ttest-merror:0.078001\n",
      "[109]\ttest-merror:0.077067\n",
      "[110]\ttest-merror:0.076600\n",
      "[111]\ttest-merror:0.076600\n",
      "[112]\ttest-merror:0.076600\n",
      "[113]\ttest-merror:0.076600\n",
      "[114]\ttest-merror:0.076600\n",
      "[115]\ttest-merror:0.076600\n",
      "[116]\ttest-merror:0.076133\n",
      "[117]\ttest-merror:0.076133\n",
      "[118]\ttest-merror:0.076133\n",
      "[119]\ttest-merror:0.076133\n",
      "[120]\ttest-merror:0.076600\n",
      "[121]\ttest-merror:0.076600\n",
      "[122]\ttest-merror:0.076600\n",
      "[123]\ttest-merror:0.076133\n",
      "[124]\ttest-merror:0.077067\n",
      "[125]\ttest-merror:0.077067\n",
      "[126]\ttest-merror:0.077067\n",
      "[127]\ttest-merror:0.077534\n",
      "[128]\ttest-merror:0.077534\n",
      "[129]\ttest-merror:0.078001\n",
      "[130]\ttest-merror:0.077534\n",
      "[131]\ttest-merror:0.078001\n",
      "[132]\ttest-merror:0.078001\n",
      "[133]\ttest-merror:0.078001\n"
     ]
    }
   ],
   "source": [
    "evallist  = [(dtest,'test')]\n",
    "mdl = xgb.train(plst, dtrain, \n",
    "                num_boost_round = n_round, \n",
    "                evals = evallist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.921999065857\n"
     ]
    }
   ],
   "source": [
    "preds = mdl.predict(dtest)\n",
    "print(np.mean(np.argmax(preds, axis=1) == teY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
